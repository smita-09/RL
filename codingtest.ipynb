{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T14:56:57.546870Z","iopub.execute_input":"2022-06-12T14:56:57.547502Z","iopub.status.idle":"2022-06-12T14:56:57.556101Z","shell.execute_reply.started":"2022-06-12T14:56:57.547464Z","shell.execute_reply":"2022-06-12T14:56:57.555323Z"},"trusted":true},"execution_count":334,"outputs":[]},{"cell_type":"markdown","source":"### Reading the data","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/open-items/Dataset_Open_Items_EN.csv\"\ndf = pd.read_csv(path, sep = ';')\n# Load the file, change the path by specifying your file's location ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:47.945151Z","iopub.execute_input":"2022-06-12T14:54:47.945534Z","iopub.status.idle":"2022-06-12T14:54:48.973427Z","shell.execute_reply.started":"2022-06-12T14:54:47.945499Z","shell.execute_reply":"2022-06-12T14:54:48.972561Z"},"trusted":true},"execution_count":306,"outputs":[]},{"cell_type":"code","source":"df.columns # check the columns of the dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:48.975620Z","iopub.execute_input":"2022-06-12T14:54:48.976190Z","iopub.status.idle":"2022-06-12T14:54:48.990335Z","shell.execute_reply.started":"2022-06-12T14:54:48.976153Z","shell.execute_reply":"2022-06-12T14:54:48.989499Z"},"trusted":true},"execution_count":307,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:48.993579Z","iopub.execute_input":"2022-06-12T14:54:48.994241Z","iopub.status.idle":"2022-06-12T14:54:49.054285Z","shell.execute_reply.started":"2022-06-12T14:54:48.994206Z","shell.execute_reply":"2022-06-12T14:54:49.053626Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"df.shape # Will get the shape of the data","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.058471Z","iopub.execute_input":"2022-06-12T14:54:49.064797Z","iopub.status.idle":"2022-06-12T14:54:49.076299Z","shell.execute_reply.started":"2022-06-12T14:54:49.064760Z","shell.execute_reply":"2022-06-12T14:54:49.075600Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"df.info() # So, there seems to be one column(target) which is object and others are float","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.080526Z","iopub.execute_input":"2022-06-12T14:54:49.082639Z","iopub.status.idle":"2022-06-12T14:54:49.111070Z","shell.execute_reply.started":"2022-06-12T14:54:49.082604Z","shell.execute_reply":"2022-06-12T14:54:49.110305Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"markdown","source":"### Total number of null values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sum() #Total number  of null values in the dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.115083Z","iopub.execute_input":"2022-06-12T14:54:49.117260Z","iopub.status.idle":"2022-06-12T14:54:49.148753Z","shell.execute_reply.started":"2022-06-12T14:54:49.117223Z","shell.execute_reply":"2022-06-12T14:54:49.147992Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"markdown","source":"### Summary of whole dataset","metadata":{}},{"cell_type":"code","source":"Summary = pd.DataFrame(df.dtypes, columns=['Dtype'])\nSummary[\"max\"] = df.max()\nSummary[\"min\"] = df.min()\nSummary[\"Null\"] = df.isnull().sum() # to get null values\nSummary[\"First\"] = df.iloc[0] # to get first value\nSummary[\"Second\"] = df.iloc[1] # to get second value\nSummary","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.153057Z","iopub.execute_input":"2022-06-12T14:54:49.155115Z","iopub.status.idle":"2022-06-12T14:54:49.262623Z","shell.execute_reply.started":"2022-06-12T14:54:49.155077Z","shell.execute_reply":"2022-06-12T14:54:49.261239Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"markdown","source":"Since we can see from the Summary of the dataset that data is not normalized and neither scaled, since the max values varies so data needs to be scaled and we do not have null values.","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"#df[\"mean\"] = df.mean(axis = 1)\n#df[\"med\"] = df.median(axis = 1)\n#df[\"std\"] = df.std(axis = 1)\n#df[\"skew\"] = df.skew(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.263862Z","iopub.execute_input":"2022-06-12T14:54:49.264370Z","iopub.status.idle":"2022-06-12T14:54:49.272610Z","shell.execute_reply.started":"2022-06-12T14:54:49.264331Z","shell.execute_reply":"2022-06-12T14:54:49.271706Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"markdown","source":"### Creating the test data","metadata":{}},{"cell_type":"code","source":"test_df = df[39001:]\ndf = df[0:39000]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.273706Z","iopub.execute_input":"2022-06-12T14:54:49.277885Z","iopub.status.idle":"2022-06-12T14:54:49.284003Z","shell.execute_reply.started":"2022-06-12T14:54:49.277842Z","shell.execute_reply":"2022-06-12T14:54:49.283113Z"},"trusted":true},"execution_count":314,"outputs":[]},{"cell_type":"markdown","source":"### Getting the target data","metadata":{}},{"cell_type":"code","source":"y = df[\"Payment_delay\"]\ndf = df.drop([\"Payment_delay\"], axis=1)\ny_test = test_df[\"Payment_delay\"]\ntest_df = test_df.drop([\"Payment_delay\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.288944Z","iopub.execute_input":"2022-06-12T14:54:49.292830Z","iopub.status.idle":"2022-06-12T14:54:49.324940Z","shell.execute_reply.started":"2022-06-12T14:54:49.292791Z","shell.execute_reply":"2022-06-12T14:54:49.324070Z"},"trusted":true},"execution_count":315,"outputs":[]},{"cell_type":"markdown","source":"Since the target data is categorical with 4 different categories in it, so it is a classification problem and we can use f1 score, confusion matrix or log lossas a metric to see how well the model is doing on the data. \nAlthough Xgboost performs well in most of the problems, we can start with that and then also use the combination of logistic regression and xgboost to see how the result changes and if there is imporvement. ","metadata":{}},{"cell_type":"code","source":"y.value_counts().plot(kind='bar', color = [\"pink\", \"green\", \"yellow\", \"red\", \"black\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.329614Z","iopub.execute_input":"2022-06-12T14:54:49.331901Z","iopub.status.idle":"2022-06-12T14:54:49.507162Z","shell.execute_reply.started":"2022-06-12T14:54:49.331862Z","shell.execute_reply":"2022-06-12T14:54:49.506476Z"},"trusted":true},"execution_count":316,"outputs":[]},{"cell_type":"markdown","source":"We can see that from that the plot the distribution of data is not normal and we have more values when the debtor pays on time and it is very unlikely that debtor takes more than 90 days to pay the bills","metadata":{}},{"cell_type":"markdown","source":"### Getting the min and max value in whole dataset","metadata":{}},{"cell_type":"code","source":"print(df.min().min()) # Min value in the whole dataset\nprint(df.max().max()) # Max value in the whole dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.511123Z","iopub.execute_input":"2022-06-12T14:54:49.511896Z","iopub.status.idle":"2022-06-12T14:54:49.566274Z","shell.execute_reply.started":"2022-06-12T14:54:49.511859Z","shell.execute_reply":"2022-06-12T14:54:49.565307Z"},"trusted":true},"execution_count":317,"outputs":[]},{"cell_type":"markdown","source":"### Getting the correltion of the data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncor = df.corr()\n#f, ax = plt.subplots(figsize=(20, 20))\n#sns.heatmap(cor, vmax=.8, square=True, annot= True);\n# See how the features are correlated so that we can remove the highest correalted features because they are redundant","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:49.567674Z","iopub.execute_input":"2022-06-12T14:54:49.568169Z","iopub.status.idle":"2022-06-12T14:54:52.090568Z","shell.execute_reply.started":"2022-06-12T14:54:49.568131Z","shell.execute_reply":"2022-06-12T14:54:52.089744Z"},"trusted":true},"execution_count":318,"outputs":[]},{"cell_type":"markdown","source":"Just to see if the data columns are correlated, if they are then the data is mostly just redundant and we can get rid of some the data columns if they are redundant, So these ['N_items_-reminded', 'number_receipts_PoL'] features are highly correlated","metadata":{}},{"cell_type":"markdown","source":"# Drop the high correlated features","metadata":{}},{"cell_type":"code","source":"df = df.drop(to_drop, axis=1)\ntest_df = test_df.drop(to_drop, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.092038Z","iopub.execute_input":"2022-06-12T14:54:52.092404Z","iopub.status.idle":"2022-06-12T14:54:52.113870Z","shell.execute_reply.started":"2022-06-12T14:54:52.092368Z","shell.execute_reply":"2022-06-12T14:54:52.112764Z"},"trusted":true},"execution_count":319,"outputs":[]},{"cell_type":"markdown","source":"### Transforming the target data","metadata":{}},{"cell_type":"code","source":"print(y.unique())","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.115769Z","iopub.execute_input":"2022-06-12T14:54:52.116193Z","iopub.status.idle":"2022-06-12T14:54:52.127948Z","shell.execute_reply.started":"2022-06-12T14:54:52.116154Z","shell.execute_reply":"2022-06-12T14:54:52.126916Z"},"trusted":true},"execution_count":320,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nle = LabelEncoder()\ny =le.fit_transform(y)\ny_test = le.fit_transform(y_test)\nprint(y[0:100]) # 1-30 is one\n#list(le.inverse_transform(['0', '1-30', '31-60', '61-90', '>90']))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.129785Z","iopub.execute_input":"2022-06-12T14:54:52.130250Z","iopub.status.idle":"2022-06-12T14:54:52.148052Z","shell.execute_reply.started":"2022-06-12T14:54:52.130213Z","shell.execute_reply":"2022-06-12T14:54:52.147215Z"},"trusted":true},"execution_count":321,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data in training and validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_val, y_train, y_val = train_test_split(df, y, stratify = y, random_state = 123, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.149678Z","iopub.execute_input":"2022-06-12T14:54:52.150320Z","iopub.status.idle":"2022-06-12T14:54:52.204650Z","shell.execute_reply.started":"2022-06-12T14:54:52.150281Z","shell.execute_reply":"2022-06-12T14:54:52.203843Z"},"trusted":true},"execution_count":322,"outputs":[]},{"cell_type":"code","source":"print(len(X_train))\nprint(len(X_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.205997Z","iopub.execute_input":"2022-06-12T14:54:52.206355Z","iopub.status.idle":"2022-06-12T14:54:52.211732Z","shell.execute_reply.started":"2022-06-12T14:54:52.206320Z","shell.execute_reply":"2022-06-12T14:54:52.210851Z"},"trusted":true},"execution_count":323,"outputs":[]},{"cell_type":"markdown","source":"### Scaling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\ntest_df = scaler.transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.212963Z","iopub.execute_input":"2022-06-12T14:54:52.213991Z","iopub.status.idle":"2022-06-12T14:54:52.303609Z","shell.execute_reply.started":"2022-06-12T14:54:52.213954Z","shell.execute_reply":"2022-06-12T14:54:52.302757Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, log_loss, f1_score","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.305045Z","iopub.execute_input":"2022-06-12T14:54:52.305381Z","iopub.status.idle":"2022-06-12T14:54:52.309674Z","shell.execute_reply.started":"2022-06-12T14:54:52.305346Z","shell.execute_reply":"2022-06-12T14:54:52.308929Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"\nlr = LogisticRegression(random_state=123, C = 0.01, penalty = 'l2')\nlr.fit(X_train, y_train)\ny_pred_l =lr.predict_proba(X_val)\n\nprint(log_loss(y_val, y_pred_l)) #0.6949971643614415 --> correlated feature drop 0.6950228858317289, 0.693083752672263, 0.682 with less data\n\ny_pred = lr.predict(X_val)\nf1score = f1_score(y_val, y_pred, average=None)\nmat = confusion_matrix(y_val, y_pred)\nprint(f1score, mat)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:52.311326Z","iopub.execute_input":"2022-06-12T14:54:52.312067Z","iopub.status.idle":"2022-06-12T14:54:56.005801Z","shell.execute_reply.started":"2022-06-12T14:54:52.312028Z","shell.execute_reply":"2022-06-12T14:54:56.004691Z"},"trusted":true},"execution_count":326,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Model","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier\nrc = RandomForestClassifier(max_features= 7, random_state= 0, n_estimators = 150)\nrc.fit(X_train, y_train)\ny_pred_l=rc.predict_proba(X_val)\n\nloss = log_loss(y_val, y_pred_l)\nprint(loss) #0.6846230417333001 --> correlated feature drop --> 0.6747265541757361, 0.62 with the removal of last 1000 data\n\ny_pred = rc.predict(X_val)\nf1score = f1_score(y_val, y_pred, average=None)\nmat = confusion_matrix(y_val, y_pred)\nprint(f1score, mat)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:54:56.007706Z","iopub.execute_input":"2022-06-12T14:54:56.008266Z","iopub.status.idle":"2022-06-12T14:55:04.114593Z","shell.execute_reply.started":"2022-06-12T14:54:56.008225Z","shell.execute_reply":"2022-06-12T14:55:04.113751Z"},"trusted":true},"execution_count":327,"outputs":[]},{"cell_type":"markdown","source":"### XgBoost Model","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n#rc = RandomForestClassifier(max_features= 7, random_state= 0, n_estimators = 150)\nxg = XGBClassifier(n_estimators = 150, learning_rate = 0.1, random_state = 0)\nxg.fit(X_train, y_train)\ny_pred_l=xg.predict_proba(X_val)\n\nloss = log_loss(y_val, y_pred_l)\nprint(loss) # 0.5901140046698274 --> correlated feature drop 0.5913514065110358 -->pretty bad results with less learning rate, 0.596 with feature engineering, without feature engineering, 0.5799 which is the best score till now\n\ny_pred = xg.predict(X_val)\nf1score = f1_score(y_val, y_pred, average=None)\nmat = confusion_matrix(y_val, y_pred)\nprint(f1score, mat) ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:55:04.115887Z","iopub.execute_input":"2022-06-12T14:55:04.116397Z","iopub.status.idle":"2022-06-12T14:56:57.449277Z","shell.execute_reply.started":"2022-06-12T14:55:04.116359Z","shell.execute_reply":"2022-06-12T14:56:57.448529Z"},"trusted":true},"execution_count":328,"outputs":[]},{"cell_type":"markdown","source":"The smaller the log loss the better the results so this shows that, xgboost perfomed better than other models\n0.5799","metadata":{}},{"cell_type":"markdown","source":"### Taking the best classifier to use it on test data","metadata":{}},{"cell_type":"code","source":"xg.classes_\n# Get all the classes ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:56:57.452605Z","iopub.execute_input":"2022-06-12T14:56:57.454360Z","iopub.status.idle":"2022-06-12T14:56:57.462699Z","shell.execute_reply.started":"2022-06-12T14:56:57.454326Z","shell.execute_reply":"2022-06-12T14:56:57.461446Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"cols = le.inverse_transform(xg.classes_)\ncols\n# Get all the orginal columns using inverse transform of label encoder","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:56:57.463961Z","iopub.execute_input":"2022-06-12T14:56:57.464382Z","iopub.status.idle":"2022-06-12T14:56:57.475517Z","shell.execute_reply.started":"2022-06-12T14:56:57.464343Z","shell.execute_reply":"2022-06-12T14:56:57.474617Z"},"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"code","source":"res = xg.predict_proba(test_df)\n# Predicting the resultant data using the best classifier that is XgBoost for this dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:56:57.477020Z","iopub.execute_input":"2022-06-12T14:56:57.477707Z","iopub.status.idle":"2022-06-12T14:56:57.503958Z","shell.execute_reply.started":"2022-06-12T14:56:57.477665Z","shell.execute_reply":"2022-06-12T14:56:57.503436Z"},"trusted":true},"execution_count":331,"outputs":[]},{"cell_type":"code","source":"res_df = pd.DataFrame(xg.predict_proba(test_df), columns=cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:56:57.505045Z","iopub.execute_input":"2022-06-12T14:56:57.505476Z","iopub.status.idle":"2022-06-12T14:56:57.528389Z","shell.execute_reply.started":"2022-06-12T14:56:57.505443Z","shell.execute_reply":"2022-06-12T14:56:57.527792Z"},"trusted":true},"execution_count":332,"outputs":[]},{"cell_type":"code","source":"res_df = res_df>0.5\nres_df","metadata":{"execution":{"iopub.status.busy":"2022-06-12T14:56:57.531806Z","iopub.execute_input":"2022-06-12T14:56:57.532253Z","iopub.status.idle":"2022-06-12T14:56:57.545413Z","shell.execute_reply.started":"2022-06-12T14:56:57.532219Z","shell.execute_reply":"2022-06-12T14:56:57.544628Z"},"trusted":true},"execution_count":333,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}